{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5qZ0rRpIDO6K",
        "outputId": "a1433bc5-31d1-4b83-f8f8-0290f7b63690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.2.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.13)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.63.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.7.3)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.4)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.14.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.2)\n",
            "Downloading langchain_google_genai-4.2.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-4.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain-google-genai pydantic langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph , START  , END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from typing import TypedDict , Annotated , Literal\n",
        "from pydantic import BaseModel , Field\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage , SystemMessage, HumanMessage"
      ],
      "metadata": {
        "id": "m5Bo2mBeEL9l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "uBLm2LVCEQui"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\"\n",
        "    , api_key= api_key)"
      ],
      "metadata": {
        "id": "ehbZlBKjES0s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class ChatState(TypedDict):\n",
        "  messages : Annotated[list[BaseMessage] , add_messages]\n",
        ""
      ],
      "metadata": {
        "id": "MKKUxh6GEaJy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_node(state : ChatState):\n",
        "  # take user query from state\n",
        "  #print(state['messages'])\n",
        "  query = state['messages']\n",
        "  #print(\"---------------------------------\")\n",
        "\n",
        "  # send to llm\n",
        "  response = model.invoke(query)\n",
        "  #print(response)\n",
        "  # response store state\n",
        "\n",
        "  #print(state['messages'] )\n",
        "  return {'messages':[response]}\n"
      ],
      "metadata": {
        "id": "gvLHqx3nm0HC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(ChatState)\n",
        "\n",
        "# nodes\n",
        "graph.add_node('chat_node' , chat_node)\n",
        "\n",
        "#Add edges\n",
        "graph.add_edge(START , 'chat_node')\n",
        "graph.add_edge('chat_node' , END)"
      ],
      "metadata": {
        "id": "8pxFIWfwEc0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c745f670-cefd-49a3-f6ee-259a814eb142"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7d0861132ab0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "ikh3VcykmtyQ",
        "outputId": "d4c2c71f-e7fc-4124-934d-fdf090800ed9"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7d0860eae8d0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAADqCAIAAAAJan3zAAAQAElEQVR4nOydB3wURfvHZ/f6XXoPJEAChP4nSAmiGOniK1UURZA/qDSpgjRFmoi+hKKoICKEonSkWEBUEKVJCxB6GiENUkhyl+t3+z53m1wul72E3dwt2WS/+jnmZrbM/XYy80zZeYQEQSAeVhAiHrbgtWYPXmv24LVmD15r9uC1Zg+3aP0wQ3v9TFFBjl6vIcxmwmggcByDAJmK4QjDMMLyFSMtToEAM5ksAQxikCVgOx7DEByCwz8CZDaWmacYsh6F4Thcv/y+5MGY5XzLEURZEm75iuCCtsviOLI/ERCJcVyIpHIsJELSuY+/QCBArgZzoX2dlaL6Y8fDojzLjxCKkViCi2UWXc16UMUiLmFVCbQGLFpbtLGEcQFmtmoNQmOkVmUxpHygneUCJluurVoTBCbACHvJrPFmRAis4tqS4KK49eliAkRYLwJ5ICpqLZTAYzDrtGaD1mzUI5EEBTWSDpkUhlyHa7QuLNDsXZ2pVSEPH7xdd++OPf0Rxzm+Oyf5WolWRfg3EL3+fmPkClyg9b4vMrJTtaERkpenhqO6RWG+9tC6bNUjU6cXvLv0CUQ1o6Zab/wgBc5/Z1kkqrvcvaw89sOD4PCaFqYaab15capPkGjIRFdWarWW7xYkRXX06j44CDGFudYb5iUHNhIPmVjX6o0q2LggydNXNPw9htU3jhixeVFKUHj9Ehp4e2kzZYHp1y3ZiBFMtP5lc5bRiAZPql9Ck7z9cWTK1ZLCXB2iDxOtU66qX5tZL+poSpq2V+xZnYHoQ1vr75eneQcIPH3FqL7ywpuhJiM6dzQf0YS21o8eGv/zdjCq3zRqJbt6shDRhJ7WUFOLpcgvWI7qNy+OaaDTEEV5Wlpn0dM6K1nTIIJtoefOnXvw4EFEnz59+mRmZiL3IJHhf+2jV43Q0xrG7dp090TscuPGDUSf7OzsR48eIbcR2FCcl03PGqHRl3mYqdm7KnPSymbIPZw6dWrr1q3Xr18PCAho3779lClTINCpUycy1cPD48SJEyqVavv27WfOnElOTobU2NjYiRMnSqVSOGD27NkwEBoaGgoXGT9+/DfffEOeCMesXLkSuZpzR/IvH3804TMaatAo1/dvazDXD+qWcuvWrWnTpnXu3Hnv3r2g2p07dxYtWoSsDwA+FyxYAEJDYOfOnfHx8aNGjVqzZg0cf+zYsQ0bNpBXEIlESVZWrVo1bNgwOAAiofJxh9BAWHNx+Rjv40FjrqCkyCQQMOxnVktCQgIUz7Fjx+I4HhIS0rp1a1Ct8mEjR47s1atXREQE+fXKlSunT5+eOnUqsox0Y1lZWdu2bSOLubsJaSB3mG2oFhpam41mRPPqj090dLRWq50+fXpMTMxzzz0XHh5uqz3sgcILFcjChQuh4Buh84qQn5+fLRWeATtCAwKxANEcSaJRTuXeQrPbFkm1bNnyiy++CAwMXLt27ZAhQyZNmgRltvJhkAqVBhxw4MCBCxcujBkzxj5VIpEgtniQqabbOaFxeIMIqdHoxgVp3bp1g3r58OHDUFMXFRVBGSdLrg1oxvft2zd8+HDQGuoZiFEqlegJAdMjQpqtFw2tw5orLPdIK0Fu4OLFi1DzQgCK9ksvvTRz5kzQEew2+2MMBoNGowkKKh1B1uv1J0+eRE+I1BtqzH1aA0IRlnC8CLkBqDHA/Ni/fz8YxYmJiWBvgOhgwEG1AOKePXsWagxoNps0aXLo0KGMjIzCwsIlS5ZALV9cXFxSQvH44Uj4BEMFrobcwMN0rW8QvUEhelqDAZ9+W4PcABgYUDPExcVBZ2/cuHEKhQLqZaHQ0nSDcXL+/Hko6VCoP/nkE2j9wKQbPHhwly5dJk+eDF979+4NFojDBcPCwgYMGLB+/Xqo4pEbMGhR1/8E0DqF3ryMXm/YMOfe5NXu6s5wheO7H9z8Vzkpjp4O9Mq1WCxSeAt2r05H9ZvbF1VRHRU0T6K/7mngxJAdn1Y1oNOzZ08zlZVvMpmgwoUeB+VZYMP5+PggNwC9JDBpKJOgdQWDnTJLkZGRmzZtojzr1KEHRgPR+/VQRBMmc7u7VqarlaYxiyIoU5nZYZ6ebhzScpYlnU7nzCSHBwAjMJRJX76XFDvMv103X0QThvPo6+ckt+zi8fzL9W7SYOuyNIkcHz6jEaIPw/GNCZ81vXFGefM87bkJTrNzZZpJb2YmNKrhWpyvZiXFvOjdqWdNF19xgu2fpUmkglemMV89UNM1ZuveT/IPEb060zWrC2stmxamQi9xzEcRqAa4YO3k5kXJGhXxVC+frv3p2fac4NCGzIzbmrAo2cDxDVHNcM2a4LO/5l36oxAsuobNpX1GBEsVnH9d4f4d5ZmfH+Vl6mFecei0Br4BLhiqdeVa95M/Prx9QaVTm8FglSow70CRTCEUiQRGc/ktcMtq89KXB5B1KTsJmYvSle3WAJjiJnP5YYR1aTz5j32WnV1BIEAmo+VgWwyJUIiRo5W2a5aeiGMmvVGrMikfGbVqSw/B01fQ9SX/qGgv5CJcqbWNv3/MzUhRa5Um+FVweZO+PMn6wgBC5VpbXwBAZeKXvpxhVRAvfQeAPAwuZP2EB2l5BljZuaSWhN2J5A/ChZhZTyAcOWotwqAnYn8kiUiMYTgSSZGXr7hxG3mHWD/katyitbuZN29ejx49+vbtizgFJytWmEMghwC5Ba81e/BaswcntYbJMBifQ1yDL9fswWvNHrzW7MHX1+zBl2v24LVmD15r9uC1Zg9ea/bgtWYPXmv24LVmD+7lGCY3TCYTrzUbcLRQI15rNuG1Zg9ea/bgXqY5OsiH+HLNJpzMdIMGDRAH4Z7WOI5nZGQgDsLBHoFQ6PA+L1fgtWYPXmv24LVmD15r9uCk1jDOhziIu/YUcisCgYCLRZuTWnO0GuFmZ5fXmjV4rdmD15o9eK3Zg9eaPXit2YPXmj04qjWX3tuNjo4md4wi80wGYmNjV69ejbgAl/qNXbt2xazgViAQGBj45ptvIo7AJa1Hjx7t71/Bg17Lli07dOiAOAKXtH766afbtGlj++rl5TV8+HDEHTg29gQ1hm3D68jIyG7duiHuwDGtocZo164dBBQKxYgRIxCnqN4OSb9TcveSUmfnSsXqzxUrC1sW6ZJObh13miED1n/Kt1DBkJko/SzPhJ1HVotbWLudb0gPvPb7qqiUykuXL0kkkpiYrraDHLZYJ7d2QRU3ZKkctnyaraejivGO1yvdfYdSKlBD6oG6Dw6q1kVvNVp/91GSTo1EEtygIxwyVBrGLXvVIPKHWSWzs8lK3RKb4XeXqUkGbK6FMdy604291hV/VdmzwWxbypfug2P/PCoJ5JCHstjyI0rzVuYF2OGnVfbGa/ERTFBrJRBiFv8CBuQXUo232Kq0/mZuUkBDYd83myCex2BnXFJwmHTgeKde6Zxq/e0HSWHNpc8Oqb/+7Biw7/MUuafg1RnUpZu6bTzz00OzCfFC06XXqLDcDIOzVGqt0+9qpZ6c32OPfXz8xAIhuvJPAWUqtaAGtRtdydRtoKkvKaAeF6PW2mS2nIN46GMGw4agNv74isLVEMiZPyVeazfgpEagbhsxTi7RqRWAdJgTsanLNcE3jEyxdKKd2BV8HeJqMIRolWse5jhvG6krZud+YHiYQ601DGnxWjMEc9Y0OmsbTXzzyBSC4O1rtrDsOE8tNq+1q7G0jdSViJNOi+tq61eG99/43VeoFrPm80/HvPUqchEYOW1HhROtn/RiqMVL5v7y60HEQax+LmiV6yfN7ds3UJ3DZfW1yWTas/f7LVs3QLh1q3b/P3p8u3bRpfcQivb/uGv9N2vEYnHbttHz5i7x9vKG+NTU5EOH9166fD4nJ6tJ48gXXxw8aOAwiO/Ry+IZfUXc0nXrVx8+eKKKm0Lxh45A7179P/3vIo1G3bp1uwnjprVq1ZZM3bpt49HffsrLexgUFBLdvuOM6fNw3FK21Gr1suUfXr58PiKi2aABw+wvWFCQ//W6VYnXr2i12s6dn35z5Nvh4fTcb5ELDqmTkIvY8O3agwf3LFkc9+H8ZYGBwXPmTUlPTyOT/jr5e0mJ6rNP174/66PExITNm9eR8V99vfL8+TPTps75dPkXIPTnX3x29twpiD/yi+Xz/VkLqhYaWResXr9x9djvv6xft+3Xn/+RiCXLP1tIJm2OX3/g4O6J46fv3XP0rbGTTvx1DIoCmRS3cmlGRnrcinVLF8elpiWfPfcPGQ/FZcbM8QlXLs6YPn/Txl2+Pn6T3h2dmUVvTwfr+DWdPjo8HBOdGruouGj3nu3Tp83t3MmyZiMm5hm1uiS/IK9RoybwVS5XjBr5FnnkqdN/Xb12mQwvWLAcDgsNsWwG0iG605Ejh/49f7przDOIDhq1Gh6hXC6HcK+eL0ABh2JrMpt27NwyccKMZ599HuKfj+2dknJ3+/ffDR3yWlFR4fETx+bMXtjaWvzHj5t6+kyp4+9r1xKgfKyMW/dUh87wdeKE6ZDbfft+mDplNo0M0bb5zE4NckrSUpORZSVj6WI7KG5LFq+wpbZrG20Le3v56HW60i8EsX//znP/nrp//x4ZERpK279ZeKMmpNCAh4fFualSWQyP2WAw2CoTICqqlUqlysy8D6nwtXHjSFtSixat7969BYFriQkikYgUGlkXmUDNc+XqJUQL5zYftdbkOhn02KhUFierUgm1gzL7LYNs4yxms3nu/GkGg/6dtydHR3fy9PCcMu0tRB+yCnagoCDPIT8ymeV5QJ1eVGzx7ymXycuTpDLbr4AnRLYWNnx8aPt6dYZr2kaFwuKaFiqExz/lzt1bt25dj1vxdcenupAx8FMDA4KQKyDzo9GWu3In8+bnF0C+kKC1WzNny7a/f4BMJlv2cYWV8wK8mpVjDmA4Rm9ehi7NmrWAwmv7c4PWAcrs0aM/VXEK1JvwaRM3LS0F/kcuomnTKIFAcP36FVvMzZuJ8KcTGBgUYm0eEhNLk6AgX7h4znaWRqMBowUaD/L/4OBQ+GmIDoTz6tfJHBjNIVUPD48+vV8EO+TXI4cuJ1xY++WKixfP2VeXlQEjDx7Prt3bipXF0CLBKdCu5jzIhiSJRAKiXLhwFi7F7L0YL08vyM/27zedPn0Srv/bbz//eGDXsGFvQIUDV27btn18/HpoJHQ63cfLPrD9VvgL69KlW1zc0gcPcqAoHDi4Z8LEUdBiI1o4V87JOF/Z0tPHB0w36OyuXLUMLKdmTaOWLFpBGiHOCA4O+WD+x2CPDxrcs2HD8A/mLYUGbcFHs0aPGbZl8943RowFow3Mkh0//OTpwcSd97uTZoKyS5fNh6fVoEHYiNfHvP7aaDIJDPw1a5aPm/AGFOoX+g14sf+gf06dIJOWL1tz6PC+JR/Pu3HjGljWvXv3Hzr0NUQPp0YF9Xq+LUvTCDP28nR6ZjwPsHVxUvRzvs8M9q+cxI/zuR5nFXZt13rAwOedatWiAQAACWlJREFUJc2Zs+jZZ55HtQ96axZwAeaso8kyGzb84CwJ+tCoFuLcrnBSrk0O6+2fGGQPnksQTt8ecE2/kedx4NtGF1PFcg9eaxdjHVOlTqLWml8cwhjC9lEJZ/1GVCtaRg6C2T4qwdchLgbG+ZytqOa1djEWY9nJkjFea/bgtWYPaq3FMgFh5ORevE8coRgJRHTWLMgUSKvltWaC0YBCm0ook6i17vFqgEbFW320ufB7rkiMGrekntyg1trbXxYSIf5+eRLiocONM0WxrwQ4S61qT4uzR3Iv/1kUGilv2Bym/MXIOfbbdJB7oDhQOdK6whCjyI3jyYRD14CotA7U4QhyL5gqTiFKt5ixu2/Frl6lvFW4g8PtMAFRlKdNv6nOz9KP/qiRh7dToarZqwXkvnlWpVWbTFS7BxCUy18xij4qhj3m0lcMVdtjdRTf8UFS3KtiVOV7UJYP55mq8B0TYAIB4eEjHDA+wNvPA1VxGQ7thWhj/vz5sbGx/fr1Q5yC97fLHrzW7MFrzR6c1Jqj7jL5cs0evNbswWvNHnx9zR58uWYPXmv24LVmD15r9uDbRvbgyzV78FqzB681e3BSa5PJxGvNBlCoq/XCUDvhpNZcLNSI15pNeK3Zg3uZ5mhHBvHlmk24l2mz2dyiBb19JmoJ3NMaDL5bt24hDsLBHgE3ne0iXms24bVmD15r9uC1Zg9Oag3jfIiDcNJpD5h9XCzanNSao9UINzu7vNaswWvNHrzW7MFrzR681uzBa80evNbswVGtufTebocOHVDZLvzwCRM0EGjfvn18fDziAlzqN0ZFReFlgNbQU/fw8Bg9ejTiCFzSeuTIkSCufUxkZGSPHj0QR+CS1gMGDAgPD7d9lUgkI0aMQNyBY2NPY8aMUSgUZDgsLIxbWy1wTOtevXpFREQgqyny2mt0PQk8Ydxu8xl1xux0jd7i6QUnNzmxOPOF/who38y2HWgwa8i6LQ65aQq5D0zpviwQwMu2hRn2wiRD0S6FXNEmok/y1RKsdPOWCs4s7DdTcdyHpdJWLziOZB54SBMZcjNusflURfo/fnj4MEOv15kJk3VTeYx04IuRriHLNh11vLWDLrav1e5rVCFsRhV32axwI6Lybqdl+w3hQkymwBtFyXq+HoLcgIu1vpugPLEnV6c2C8W41EvsGaTwD/NCXMBgMDxKVykL1PoSg8lA+AaJ3pjbGLkUV2odvyRN9cgo85U07cy1je8roinS3L+Wq1ebwlvKBo2n7Q7OGa7ROu226pdvc8QKUbOuYaiuACU95UwW1ObvLItErsAFWudla3fFZYS2CfQL9UB1jnsJOao8zbsrm6EaU1Otb18qPLYtr23fCFR3yb6dl39POXl1TeWukX2dlaI+tr2OCw2Etgjwb+L51cya7sJZI60PfJ0V2tplriRrM6FRAVJPcfySVFQDmGu9bVkaNIb+DX1Q/aBpTEO10nzyx4eIKQy1Tr+tKi4w1iWr43EIiPBKPF2MmMJQ6z935co8xaieERThB0MIv+/IQYxgqLWq0NSkcyiqraxY+/q+w/9FbsAjQA6DMIgRTLQ+siVLKMIpHTjXecLbBRl0RGGuHtGHiV45aVqxvP46lRAIsTO/5CP6MJFMU2L2beSuEUiTyfjr7+tv3jlVWJgT0bh9t5hXWrd4hkxauLxfv17jStSFv/25USKWtWjedVD/97y8LPuo5zxM2blvyYPc1GaRHXvHjkXuRCgR5mfpEH2YlGuTEXkHuUvrH3+K+/vMjmdjXpk/80C7Nj237px7NfFPMkkgEJ34ZzuG4Uvm/TZ76u7Ue1eOHv8WWd4uNWzcOt3HO2j21F3/6TsZjlEq85DbEMuEJUVMVkzQ1lpTbIAxYJmnW7Q2GHQXEn7u2X30012GKuTeMR0Hdvi/fsdOfGc7IMAvrHfsGJnME4pzi2ZdMzItLzpeu3G8sOjBwP4zfH1CQoIih7w0S6NVIrchlIhMJibO6WhrbTDh7vPLdj/rptGoj2oWY4tp2uSp7AdJJeoi8mtYw1a2JJnMS6tTQSAv/75YJPXzLbWLvDwDfLyDkdsQCDBkRgygXV9LpG70gafVWLT7auM4h3ilKh+KuTVIUaDUmmKxRG4fIxJKkdswmc1Ubhqqh77WMiH8MZQotQpP1/8esqEbNmhegF+4fbyvd1WTUnKZl06nto/R6hiawI+DQWMAkxfRh4kdArN5qmy1O7QO9G8kElmcDoE5QcYoVQUw6iupWGwd8PUJNRi0UNWEBluGPTOz7xQrc5Hb0GuMcm8mmyAxeT4yD4HykRa5AdC0b493jh3/LuVegsGoBwtkQ/yU/T9V0wNs0+o5oVC858ByvV5bVJy7ffeH8tIKxy0YdabgMCbjE0zKdVhzWfIVd/2R9ug+qkFo1PG/t95NPi+VejQJb/fKoPlVnyKTerw1ctXPv3354bKe0EiC2Xfp6lH3ebElzMRzQ/0RfRjOy3w5I6l594YSWb0bfkq/8kCn1DKbgWQ4puEdIEy/zHwkl7uo8tUtOzKcVmU4rPHC2JBdKzKqOOCb+Mn3M29WjjebTfCXJBBQ33fu9H0eCpdNPvx5csuff291kujUGdZ7k7bbTHUHsu8U4DjWfWgQYgTzud0dK+6pionm3cIpU4uVedAroUzSG3RiEbWHQz9fVy4s0WiUzjqQJepihZx6kZC3V5CzonDjj9TOfX0792VSWaMazqN/PSvJv7FXcDOG9+YWd05lyOTEqPlNEFNqNAb99ieNclOYzwlxiLQrWdCFqYnQqIZai8Xi4bMaJh6r0exy7Sf1QpY6X1fz5TguWPekKtTHL04PiPQOaeaH6hwp/2ZpVbpJK2rBuicSyM2mxfeFIkFU90aorlCcW5KZmCuR42MXuWaxkSvXqYJlkp9lEHsKGkeHcLqb8yiz6EFyoclgBlO6l+vWYrt4/XVhnubQuuziAjMmQBKFWO4n9Q2Vu2liwbUU52mKcpSaIq1RZ0YEEdxY8vKUcORS3PUu6bHvs7OStSVKk7lsa6by+1TnMJfyGPIlD0eoXzigHOWuKh6zvvkgECMvX2HTaEVMv0DkBth4b1ev0qtUZpO51OYhHQ1j5LszpZmwvvFSnlrReTBmfXuGsGbWFlmebPcUyZOxsh9lCVmfEOmd2mwNl96ATLF8CnHkG8JGjcdJ38Ycpf4u82AfXmv24LVmD15r9uC1Zg9ea/b4HwAAAP//DI8ilQAAAAZJREFUAwA8Dsm5roRmUgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = graph.compile()"
      ],
      "metadata": {
        "id": "SovI2kPJpDjL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_query = input(\"Enter your query : \")\n",
        "\n",
        "  if user_query.strip().lower() in ['exit' , 'bye' , 'end' , 'ok bye' ] :\n",
        "    break\n",
        "\n",
        "  initial_state = {'messages' :  [\n",
        "        SystemMessage(content=\"You are a helpful assistant.Always answer in less than 50 words \"),\n",
        "        HumanMessage(content=user_query)\n",
        "    ]}\n",
        "  response = workflow.invoke(initial_state);\n",
        "\n",
        "  print(f\"You asked : {response['messages'][1].content} \\nBot Reply : {response['messages'][2].content}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmibqDxD1B7i",
        "outputId": "56efa148-d06f-4b3a-852b-b7c6faee71ec"
      },
      "execution_count": 67,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query : ok bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### problem no memory !\n",
        "chatbot is being reinvoked everytime with the while loop intereation so new invoke --> new state --> memory fkd everthing cleared\n"
      ],
      "metadata": {
        "id": "_WLb3xIw2HQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution --> persistance ---> at the end node of graph dont erase state ... store it somewhere"
      ],
      "metadata": {
        "id": "RZ1VbdMn3r5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Add checkpointer and pass to compile()\n",
        "2. add thread config and pass to invoke()\n",
        "\n",
        "\n",
        "Uses RAM , use database in production"
      ],
      "metadata": {
        "id": "AYWmXEPE-IFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver"
      ],
      "metadata": {
        "id": "oX3r4fv-2HDx"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = MemorySaver()\n",
        "persisted_graph = StateGraph(ChatState)\n",
        "\n",
        "# nodes\n",
        "persisted_graph.add_node('chat_node' , chat_node)\n",
        "\n",
        "#Add edges\n",
        "persisted_graph.add_edge(START , 'chat_node')\n",
        "persisted_graph.add_edge('chat_node' , END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtmrEo3m8I5u",
        "outputId": "80f88965-19ea-447b-b96e-363084ffb826"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7d0860f78590>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = persisted_graph.compile(checkpointer = checkpointer)"
      ],
      "metadata": {
        "id": "SoIRtcmp86N7"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread_id =\"1\"\n",
        "while True:\n",
        "  user_query = input(\"Enter your query : \")\n",
        "\n",
        "  if user_query.strip().lower() in ['exit' , 'bye' , 'end' , 'ok bye' ] :\n",
        "    break\n",
        "\n",
        "  initial_state = {'messages' :  [\n",
        "        SystemMessage(content=\"You are a helpful assistant. Always answer in less than 50 words \"),\n",
        "        HumanMessage(content=user_query)\n",
        "    ]}\n",
        "\n",
        "  config = {'configurable' :{'thread_id' : thread_id} }\n",
        "\n",
        "  response = workflow.invoke(initial_state , config=config);\n",
        "\n",
        "  print(f\"You asked : {response['messages'][-2].content} \\nBot : {response['messages'][-1].content}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cr4FwUBU8-sB",
        "outputId": "49c58460-c8f4-439a-c653-35d557a8fb43"
      },
      "execution_count": 85,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query : hi i am shagun\n",
            "You asked : hi i am shagun \n",
            "Bot : Hi Shagun! It's good to hear from you again. How can I help you today?\n",
            "Enter your query : i am sad todaay \n",
            "You asked : i am sad todaay  \n",
            "Bot : I'm sorry to hear you're feeling sad today, Shagun. Is there anything I can do to help, or something you'd like to talk about?\n",
            "Enter your query : what was my name \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ChatGoogleGenerativeAIError",
          "evalue": "Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 29.191846459s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   3046\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m             response: GenerateContentResponse = self.client.models.generate_content(\n\u001b[0m\u001b[1;32m   3048\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5473\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5474\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5475\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4214\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4215\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     )\n\u001b[0;32m-> 1396\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1397\u001b[0m     response_body = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mretry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtenacity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetrying\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mretry_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1208\u001b[0m       )\n\u001b[0;32m-> 1209\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m       return HttpResponse(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 29.191846459s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}}",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-373601649.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'configurable'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'thread_id'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mthread_id\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You asked : {response['messages'][-2].content} \\nBot : {response['messages'][-1].content}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0minterrupts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInterrupt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2647\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-209405266.py\u001b[0m in \u001b[0;36mchat_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# send to llm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m#print(response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# response store state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[1;32m   2533\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m     def _get_ls_params(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   3049\u001b[0m             )\n\u001b[1;32m   3050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3051\u001b[0;31m             \u001b[0m_handle_client_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_response_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/chat_models.py\u001b[0m in \u001b[0;36m_handle_client_error\u001b[0;34m(e, request)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error calling model '{model_name}' ({e.status}): {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mChatGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mChatGoogleGenerativeAIError\u001b[0m: Error calling model 'gemini-2.5-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 29.191846459s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '29s'}]}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.get_state(config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbdJCdiP-mLF",
        "outputId": "4e0861dd-1606-489b-aa4f-694416d850d2"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='0c175ce7-f56e-49b9-a867-939161cc9213'), HumanMessage(content='hi i am shagun', additional_kwargs={}, response_metadata={}, id='937ba7e7-d039-4ec6-9236-105d884a25c5'), AIMessage(content='Hi Shagun! Nice to meet you. How can I help you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a01-117e-7683-8963-fcea02eb7724-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 40, 'total_tokens': 62, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 24}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='d2fb61e2-6144-45eb-876e-4185b64a4cee'), HumanMessage(content='what is my name ', additional_kwargs={}, response_metadata={}, id='0258d4d6-45bb-421e-a22c-7235a6a041af'), AIMessage(content='Your name is Shagun.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a01-2768-7930-a086-a83e031d7f40-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 61, 'output_tokens': 43, 'total_tokens': 104, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 37}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='caeb7ccc-497f-4e98-8338-415b46f97857'), HumanMessage(content='hi i am shagun', additional_kwargs={}, response_metadata={}, id='cb72554a-ea12-486b-b676-e80fb0349083'), AIMessage(content='Hi Shagun! Welcome back. How can I assist you today?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a03-4755-72e1-a026-c37acb2ee62a-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 90, 'output_tokens': 39, 'total_tokens': 129, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 25}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='f041ac46-7083-4573-8f3e-9c4a6a03ee6e'), HumanMessage(content='what was my name ', additional_kwargs={}, response_metadata={}, id='1b38df18-08ec-413e-8c5e-9ad82b6bf091'), AIMessage(content='Your name was Shagun.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a03-6355-7da0-b0ed-d9fafce18b6b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 127, 'output_tokens': 44, 'total_tokens': 171, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 38}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='d50fc1c6-3b8f-493f-b62c-333053c77ca1'), HumanMessage(content='hi i am shagun', additional_kwargs={}, response_metadata={}, id='bd05dc8c-c886-479a-96f9-47fca0b1a2c3'), AIMessage(content=\"Hi Shagun! It's good to hear from you again. How can I help you today?\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a04-a92d-7293-ab92-327811837064-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 156, 'output_tokens': 47, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 26}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='e01ef988-7996-46f0-b157-37c4982d7bd1'), HumanMessage(content='i am sad todaay ', additional_kwargs={}, response_metadata={}, id='efab1856-4818-46ff-8515-c370deae0684'), AIMessage(content=\"I'm sorry to hear you're feeling sad today, Shagun. Is there anything I can do to help, or something you'd like to talk about?\", additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c7a04-dd6b-73d0-a1ad-2313ce83fb05-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 201, 'output_tokens': 63, 'total_tokens': 264, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 28}}), SystemMessage(content='You are a helpful assistant. Always answer in less than 50 words ', additional_kwargs={}, response_metadata={}, id='d2d6a122-2915-48c2-a598-4976db9d3688'), HumanMessage(content='what was my name ', additional_kwargs={}, response_metadata={}, id='2e4a39d1-9804-4f98-8935-dc928d1304ad')]}, next=('chat_node',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f10e307-8618-6630-8012-6cae1794d8d6'}}, metadata={'source': 'loop', 'step': 18, 'parents': {}}, created_at='2026-02-20T07:47:56.607213+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f10e307-8615-6438-8011-61b3293acf5d'}}, tasks=(PregelTask(id='f97a34e6-ad98-75c9-8343-d10386c66ffa', name='chat_node', path=('__pregel_pull', 'chat_node'), error='ChatGoogleGenerativeAIError(\"Error calling model \\'gemini-2.5-flash\\' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {\\'error\\': {\\'code\\': 429, \\'message\\': \\'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\\\\\\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\\\\\\\nPlease retry in 29.191846459s.\\', \\'status\\': \\'RESOURCE_EXHAUSTED\\', \\'details\\': [{\\'@type\\': \\'type.googleapis.com/google.rpc.Help\\', \\'links\\': [{\\'description\\': \\'Learn more about Gemini API quotas\\', \\'url\\': \\'https://ai.google.dev/gemini-api/docs/rate-limits\\'}]}, {\\'@type\\': \\'type.googleapis.com/google.rpc.QuotaFailure\\', \\'violations\\': [{\\'quotaMetric\\': \\'generativelanguage.googleapis.com/generate_content_free_tier_requests\\', \\'quotaId\\': \\'GenerateRequestsPerDayPerProjectPerModel-FreeTier\\', \\'quotaDimensions\\': {\\'location\\': \\'global\\', \\'model\\': \\'gemini-2.5-flash\\'}, \\'quotaValue\\': \\'20\\'}]}, {\\'@type\\': \\'type.googleapis.com/google.rpc.RetryInfo\\', \\'retryDelay\\': \\'29s\\'}]}}\")', interrupts=(), state=None, result=None),), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}